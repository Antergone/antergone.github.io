{"meta":{"title":"Antergone","subtitle":"Progress is the activity of today and the assurance of tomorrow.","description":"盼盼的杂记,记录本人技术领域的点滴成长,主要包括网络安全,信息安全,Java,Scala,Golang,Docker等技术相关文章。","author":"Antergone","url":"http://www.antergone.com"},"pages":[{"title":"categories","date":"2017-08-23T18:27:58.000Z","updated":"2017-08-23T18:28:41.000Z","comments":false,"path":"categories/index.html","permalink":"http://www.antergone.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2017-08-23T18:27:50.000Z","updated":"2017-08-23T18:28:23.000Z","comments":false,"path":"tags/index.html","permalink":"http://www.antergone.com/tags/index.html","excerpt":"","text":""},{"title":"读书","date":"2017-08-23T11:42:49.000Z","updated":"2017-08-23T11:23:50.000Z","comments":true,"path":"reading/index.html","permalink":"http://www.antergone.com/reading/index.html","excerpt":"","text":""},{"title":"关于","date":"2017-08-23T11:42:49.000Z","updated":"2017-08-23T11:23:50.000Z","comments":true,"path":"about/index.html","permalink":"http://www.antergone.com/about/index.html","excerpt":"","text":"大家好，我是XXX。欢迎来到我的个人技术博客。 这里用markdown写下你的简介，就跟平时写md一样就可以了。"}],"posts":[{"title":"在本地使用Docker搭建ES5.6.1集群","slug":"在本地使用Docker搭建ES5.6.1集群","date":"2017-11-17T05:36:35.000Z","updated":"2017-11-17T05:37:18.792Z","comments":true,"path":"20171117/在本地使用Docker搭建ES5.6.1集群.html","link":"","permalink":"http://www.antergone.com/20171117/在本地使用Docker搭建ES5.6.1集群.html","excerpt":"","text":"docker run -d --name es0 -p 9200:9200 -p 9300:9300 \\-e xpack.security.enabled=false \\-e xpack.monitoring.enabled=false \\-e xpack.watcher.enabled=false \\-e xpack.ml.enabled=false \\docker.elastic.co/elasticsearch/elasticsearch:5.6.1docker run -d --name es1 --link=es0 -p 9201:9200 -p 9301:9300 \\-e xpack.security.enabled=false \\-e xpack.monitoring.enabled=false \\-e xpack.watcher.enabled=false \\-e xpack.ml.enabled=false \\-e discovery.zen.ping.unicast.hosts=es0:9300 \\docker.elastic.co/elasticsearch/elasticsearch:5.6.1docker run -d --name es2 --link=es0 -p 9202:9200 -p 9302:9300 \\-e xpack.security.enabled=false \\-e xpack.monitoring.enabled=false \\-e xpack.watcher.enabled=false \\-e xpack.ml.enabled=false \\-e discovery.zen.ping.unicast.hosts=es0:9300 \\docker.elastic.co/elasticsearch/elasticsearch:5.6.1","categories":[{"name":"Docker","slug":"Docker","permalink":"http://www.antergone.com/categories/Docker/"},{"name":"技术杂记","slug":"Docker/技术杂记","permalink":"http://www.antergone.com/categories/Docker/技术杂记/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.antergone.com/tags/Docker/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.antergone.com/tags/ElasticSearch/"}]},{"title":"ElasticJob动态分片","slug":"ElasticJob动态分片","date":"2017-03-21T08:25:39.000Z","updated":"2017-08-23T18:11:38.000Z","comments":true,"path":"20170321/ElasticJob动态分片.html","link":"","permalink":"http://www.antergone.com/20170321/ElasticJob动态分片.html","excerpt":"","text":"在上一次写ElasticJob的文章( https://www.antergone.com/57.html )的时候最后留了一个设想，动态的改变ElasticJob的执行计划，经过测试发现还真有这么一个方法。 应用场景数据库每天都会进入80 - 100万的数据，需要用ElasticJob做任务调度器将数据转移到HBase。 原有方案思路预估数据量120万注册任务片数 240片下面是SQL select * from op_log limit [5000 * #&#123;shardingNum - 1&#125;], 5000;或select * from op_log where id &gt; [5000 * #&#123;shardingNum - 1&#125;] and id &lt;= [#&#123;shardingNum&#125; * 5000]; 问题如果原有数据并没有到120万，那么后面的SQL根本拿不到数据，浪费数据资源。如果id是自增的，上面的两种SQL，第二个SQL可能会好点会命中索引，第一条根本不会命中，到后面就相当于全表扫了，多线程跑，数据库直接就挂了。 动态分片优化后思路在任务执行前10分钟统计出数据库现有数据量 select min(id), max(id) from op_log 片数为 [(max - min) / 5000 ] + 1重新动态注册ElasticJob如果原数据不到120万片数一定少于240。根据实际数据算出片数，再结合执行上次文章的线程优化，任务有很大的优化空间。 select * from op_log where id &gt; [5000 * #&#123;shardingNum - 1&#125;] and id &lt;= [#&#123;shardingNum&#125; * 5000]; 具体操作任务初始化的代码 @Configuration@ConditionalOnBean(ElasticJobConfig.class)@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class Jobs implements CommandLineRunner &#123; private final ZookeeperRegistryCenter regCenter; private final TestJob testJob; @Override public void run(String... args) throws Exception &#123; testJob(); &#125; private void testJob() &#123; LiteJobConfiguration jobConfig = simpleJobConfigBuilder(\"测试任务\", testJob.getClass(), 1, \"0/5 * * * * ? \").overwrite(true).build(); new SpringJobScheduler(testJob, regCenter, jobConfig).init(); &#125; private static LiteJobConfiguration.Builder simpleJobConfigBuilder( final String jobName, final Class&lt;? extends SimpleJob&gt; jobClass, final int shardingTotalCount, final String cron) &#123; return LiteJobConfiguration.newBuilder(new SimpleJobConfiguration( JobCoreConfiguration.newBuilder(jobName, cron, shardingTotalCount).build(), jobClass.getCanonicalName())); &#125;&#125; 重新动态注册的代码: private void _dynamicTestJob() &#123; int shardingTotal = 10; //todo 动态获取数据库数据量 //todo 算出片数 LiteJobConfiguration jobConfig = simpleJobConfigBuilder(\"测试任务\", testJob.getClass(), shardingTotal, \"0/5 * * * * ? \").overwrite(true).build(); new SpringJobScheduler(testJob, regCenter, jobConfig).init();&#125; 这里注意：任务名一定不能错, overwrite(true) 一定不能少。 重新注册一遍就会动态啦~执行计划已经可以改变了~","categories":[{"name":"Java","slug":"Java","permalink":"http://www.antergone.com/categories/Java/"},{"name":"技术杂记","slug":"Java/技术杂记","permalink":"http://www.antergone.com/categories/Java/技术杂记/"}],"tags":[{"name":"ElasticJob","slug":"ElasticJob","permalink":"http://www.antergone.com/tags/ElasticJob/"},{"name":"Java","slug":"Java","permalink":"http://www.antergone.com/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.antergone.com/tags/SpringBoot/"}]},{"title":"Linux 利用Google Authenticator实现ssh登录双因素认证","slug":"Linux 利用Google Authenticator实现ssh登录双因素认证","date":"2017-03-20T09:31:23.000Z","updated":"2017-08-23T17:44:38.000Z","comments":true,"path":"20170320/Linux 利用Google Authenticator实现ssh登录双因素认证.html","link":"","permalink":"http://www.antergone.com/20170320/Linux 利用Google Authenticator实现ssh登录双因素认证.html","excerpt":"","text":"前因前些天因为工作需要在阿里云弄了一批VPC机器，刚开始是每个人都给了账号密码让其登录控制，又因为各种问题，收回了权限。为了控制登录权限，所以启动了跳板机 （ http://www.jumpserver.org/ ）后来又一个开发开始质疑，VPC其他的机器你可以防火墙禁止外界登录，那你跳板机最该要开一个ssh吧？如果这个机器的账号密码被知道了，那不是GG了好吧，我就想到了这个坑办法。OTP，一次性密码~ 目的让登录的时不仅需要linux的账号密码，还需要一个动态的口令。 操作手机下载 Google Authenticator 安装chrony安装这个的原因是因为Google的OTP算法其中有一个因素是时间 [root@localhost ~]# yum install -y chrony[root@localhost ~]# vim /etc/chrony.confserver 0.cn.pool.ntp.orgserver 1.cn.pool.ntp.orgserver 2.cn.pool.ntp.orgserver 3.cn.pool.ntp.org[root@localhost ~]# systemctl restart chronyd[root@localhost ~]# chronyc sources[root@localhost ~]# yum install -y git automake libtool pam-devel[root@localhost ~]# git clone https://github.com/google/google-authenticator-libpam.git[root@localhost ~]# cd google-authenticator-libpam/[root@localhost google-authenticator-libpam]# ./bootstrap.sh[root@localhost google-authenticator-libpam]# ./configure[root@localhost google-authenticator-libpam]# make &amp;&amp; make install[root@localhost google-authenticator-libpam]# google-authenticator[root@localhost google-authenticator-libpam]# cd ~[root@localhost ~]# vim /etc/pam.d/sshdauth required pam_google_authenticator.so no_increment_hotp[root@localhost ~]# vim /etc/ssh/sshd_configasswordAuthentication yesChallengeResponseAuthentication yesUsePAM yes[root@localhost ~]# systemctl restart sshd [root@localhost ~]# google-authenticatorDo you want authentication tokens to be time-based (y/n) y#你想做的认证令牌是基于时间的吗？Warning: pasting the following URL into your browser exposes the OTP secret to Google: https://www.google.com/chart?chs=200x200&amp;chld=M|0&amp;cht=qr&amp;chl=otpauth://totp/root@localhost.localdomain%3Fsecret%3DN4HLEJOQHT27VCR6RX66WXB2SY%26issuer%3Dlocalhost.localdomain[这里会有一个很大的二维码]Your new secret key is: N4HLEJOQHT27VCR6RX66WXB2SY#这个key就是加密串，如果你有多个设备，需要把这个保存下，方便以后添加认证设备Your verification code is 299695#输入手机上Google Authenticator的code Your emergency scratch codes are:#下面这些key是紧急安全码，假如你的手机丢了，紧急登录用的。 44477086 92790948 29251218 26350870 30696065Do you want me to update your \"/root/.google_authenticator\" file? (y/n) y#你希望我更新你的“/root/.google_authenticator”文件吗(y/n)？Do you want to disallow multiple uses of the same authenticationtoken? This restricts you to one login about every 30s, but it increasesyour chances to notice or even prevent man-in-the-middle attacks (y/n) y#你希望禁止多次使用同一个验证令牌吗?这限制你每次登录的时间大约是30秒， 但是这加大了发现或甚至防止中间人攻击的可能性(y/n)?By default, a new token is generated every 30 seconds by the mobile app.In order to compensate for possible time-skew between the client and the server,we allow an extra token before and after the current time. This allows for atime skew of up to 30 seconds between authentication server and client. If youexperience problems with poor time synchronization, you can increase the windowfrom its default size of 3 permitted codes (one previous code, the currentcode, the next code) to 17 permitted codes (the 8 previous codes, the currentcode, and the 8 next codes). This will permit for a time skew of up to 4 minutesbetween client and server.Do you want to do so? (y/n) y#默认情况下，令牌保持30秒有效;为了补偿客户机与服务器之间可能存在的时滞，我们允许在当前时间前后有一个额外令牌。如果你在时间同步方面遇到了问题， 可以增加窗口从默认的3个可通过验证码增加到17个可通过验证码，这将允许客户机与服务器之间的时差增加到4分钟。你希望这么做吗(y/n)?If the computer that you are logging into is not hardened against brute-forcelogin attempts, you can enable rate-limiting for the authentication module.By default, this limits attackers to no more than 3 login attempts every 30s.Do you want to enable rate-limiting? (y/n) y#如果你登录的那台计算机没有经过固化，以防范运用蛮力的登录企图，可以对验证模块启用尝试次数限制。默认情况下，这限制攻击者每30秒试图登录的次数只有3次。 你希望启用尝试次数限制吗(y/n)? 登录验证注意，第一次登录可能会出现登录失败的情况，查看日志信息显示错误如下： [root@localhost ~]# tail -n10 /var/log/secure...Dec 31 09:42:46 localhost sshd[2393]: PAM unable to dlopen(/usr/lib64/security/pam_google_authenticator.so): /usr/lib64/security/pam_google_authenticator.so: cannot open shared object file: No such file or directoryDec 31 09:42:46 localhost sshd[2393]: PAM adding faulty module: /usr/lib64/security/pam_google_authenticator.so...[root@localhost ~]# ln -sv /usr/local/lib/security/pam_google_authenticator.so /usr/lib64/security/pam_google_authenticator.so\"/usr/lib64/security/pam_google_authenticator.so\" -&gt; \"/usr/local/lib/security/pam_google_authenticator.so\" 登录方法一定要将这个放在第一个！","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://www.antergone.com/categories/DevOps/"},{"name":"技术杂记","slug":"DevOps/技术杂记","permalink":"http://www.antergone.com/categories/DevOps/技术杂记/"}],"tags":[{"name":"Google","slug":"Google","permalink":"http://www.antergone.com/tags/Google/"},{"name":"Linux","slug":"Linux","permalink":"http://www.antergone.com/tags/Linux/"},{"name":"ssh","slug":"ssh","permalink":"http://www.antergone.com/tags/ssh/"}]},{"title":"SpringBoot中使用ElasticJob","slug":"SpringBoot中使用ElasticJob","date":"2017-03-17T08:26:45.000Z","updated":"2017-08-23T18:47:53.000Z","comments":true,"path":"20170317/SpringBoot中使用ElasticJob.html","link":"","permalink":"http://www.antergone.com/20170317/SpringBoot中使用ElasticJob.html","excerpt":"","text":"去年十一月份给Elastic Job提了一个PR( https://github.com/dangdangdotcom/elastic-job/pull/181 )，使SpringBoot可以直接使用注解将ElasticJob跑起来。用了一段时间，在这里也写出一个简短的小记录。 ElasticJob版本Version 2.0.3及以上 创建Zookeeper注册中心@Data@Configuration@ConfigurationProperties(prefix = \"com.antergone.ejob\")@ConditionalOnProperty(prefix = \"com.antergone.ejob\", name = \"enabled\", havingValue = \"true\")public class ElasticJobConfig &#123; private String zkNodes; private String namespace; @Bean public ZookeeperConfiguration zkConfig() &#123; return new ZookeeperConfiguration(zkNodes, namespace); &#125; @Bean(initMethod = \"init\") public ZookeeperRegistryCenter regCenter(ZookeeperConfiguration config) &#123; return new ZookeeperRegistryCenter(config); &#125; 定义任务定义任务也比较简单，方法也比较多。前期我是直接把每个任务都做成一个Bean（代码段1），后来看了源码发现这个实例只需要init向zk注册一下任务信息，所以就换用了第二种方式（代码段2）。 代码段1： @Configuration@ConditionalOnBean(JobConfig.class)@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class ElasticJobs &#123; private final ZookeeperRegistryCenter regCenter; private final SampleJob sampleJob; private final SampleJobListener sampleJobListener; @Bean(initMethod = \"init\") private SpringJobScheduler sampleJob() &#123; LiteJobConfiguration jobConfig = LiteJobConfiguration.newBuilder(new SimpleJobConfiguration( JobCoreConfiguration.newBuilder(\"Sample Job\", \"0 0 22 * * ? \", 5).build(), SampleJob.class.getCanonicalName())).overwrite(true).build(); return new SpringJobScheduler(sampleJob, regCenter, jobConfig, sampleJobListener).init(); &#125;&#125; 代码段2： @Configuration@ConditionalOnBean(JobConfig.class)@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class ElasticJobs &#123; private final ZookeeperRegistryCenter regCenter; private final SampleJob sampleJob; private final SampleJobListener sampleJobListener; @PostConstruct public void init() &#123; _sampleJob(); &#125; private void _sampleJob() &#123; LiteJobConfiguration jobConfig = LiteJobConfiguration.newBuilder(new SimpleJobConfiguration( JobCoreConfiguration.newBuilder(\"Sample Job\", \"0 0 22 * * ? \", 5).build(), SampleJob.class.getCanonicalName())).overwrite(true).build(); new SpringJobScheduler(sampleJob, regCenter, jobConfig, sampleJobListener).init(); &#125;&#125; 修改任务执行线程池定义Handlerpublic final class SampleJobHandler implements ExecutorServiceHandler &#123; public SampleJobHandler() &#123; &#125; @Override public ExecutorService createExecutorService(String jobName) &#123; return ExecutorManager.SAMPLE_JOB_EXECUTOR; &#125;&#125; 任务定义时带入private void _sampleJob() &#123; LiteJobConfiguration jobConfig = LiteJobConfiguration.newBuilder(new SimpleJobConfiguration( JobCoreConfiguration.newBuilder(\"Sample Job\", \"0 0 22 * * ? \", 5) .jobProperties(\"executor_service_handler\", ExecutorManager.SAMPLE_JOB_EXECUTOR).build(), SampleJob.class.getCanonicalName())).overwrite(true).build(); new SpringJobScheduler(sampleJob, regCenter, jobConfig, sampleJobListener).init();&#125; 这样就可以控制每个任务执行节点上的线程池。 总结用了一段时间的ElasticJob，官方API都是使用原有的XML进行配置。之前有跟一个开发者聊过，他们甚至没有考虑过做SpringBoot的尝试。SpringBoot上使用ElasticJob还有很多坑没有踩，还需要更多更多的实践和探究。我上面的demo也只能说可以用，并没有做什么优化和创新。 设想ElasticJob官方API中讲述的XML配置和我上述说的bean初始化的方式，都是项目在初始化启动的时候将任务信息注册到zk的，在执行中是无法改变的。那我们常有一类定时任务，比如做每个用户的统计，用户是一个增量数据，如果我们每片的数量是100， 那么随着用户数量的增长，片数也要动态的增长。现在很多的解决方案都是预估未来的量，但这并不靠谱。我们能否可以在每天任务执行前一段时间先统计出所有的用户数，然后覆盖原有任务的信息。最近我也在研究这个东西，不知道是否可实现，有好方法的小伙伴可以pm我哦~","categories":[{"name":"Java","slug":"Java","permalink":"http://www.antergone.com/categories/Java/"},{"name":"技术杂记","slug":"Java/技术杂记","permalink":"http://www.antergone.com/categories/Java/技术杂记/"}],"tags":[{"name":"ElasticJob","slug":"ElasticJob","permalink":"http://www.antergone.com/tags/ElasticJob/"},{"name":"Java","slug":"Java","permalink":"http://www.antergone.com/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.antergone.com/tags/SpringBoot/"}]},{"title":"Docker使用Calico自定义网络","slug":"Docker使用Calico自定义网络","date":"2017-03-15T17:43:14.000Z","updated":"2017-08-23T17:16:51.000Z","comments":true,"path":"20170316/Docker使用Calico自定义网络.html","link":"","permalink":"http://www.antergone.com/20170316/Docker使用Calico自定义网络.html","excerpt":"","text":"简介略~ 技术博客不扯没用的，show U my code ~ 操作 calico版本: v1.0.2 docker版本: v1.13.1 下载# calico 的release地址 https://github.com/projectcalico/calicoctl/releaseswget https://github.com/projectcalico/calicoctl/releases/download/v1.0.2/calicoctl 准备ETCD提示: 这里只是简单的做一个可用的，建议做集群。 yum install -y etcdnohup etcd -listen-peer-urls http://10.10.10.10:2380 -listen-client-urls http://10.10.10.10:2379,http://127.0.0.1:2379 -advertise-client-urls http://10.10.10.10:2379 &amp; 配置docker添加如下配置, 如果没有这个文件，就新建~ vim /etc/docker/daemon.json &#123; \"cluster-store\": \"etcd://10.10.10.10:2379/calico\", &#125; 配置calico这里注意下，calico新版的命令都变了。 下面的操作都是在一台机器上处理就可以了，不需要每台机器都处理 将当前主机加入calicocalicoctl node run --ip=10.10.10.10 注意: 每个机器都要加入，并且写自己的IP 这里下载需要从docker hub上下载一个node镜像，所以有点慢，可以使用镜像仓库 所有机器执行成功之后可以看当前的节点情况 [root@izuf61ygoynhba5hbyzuv0z ~]# calicoctl node statusCalico process is running.IPv4 BGP status+--------------+-------------------+-------+------------+-------------+| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |+--------------+-------------------+-------+------------+-------------+| 10.10.10.13 | node-to-node mesh | up | 2017-03-13 | Established || 10.10.10.11 | node-to-node mesh | up | 2017-03-13 | Established || 10.10.10.12 | node-to-node mesh | up | 2017-03-13 | Established || 10.10.10.15 | node-to-node mesh | up | 2017-03-13 | Established || 10.10.10.16 | node-to-node mesh | up | 2017-03-13 | Established || 10.10.10.14 | node-to-node mesh | up | 2017-03-13 | Established |+--------------+-------------------+-------+------------+-------------+IPv6 BGP statusNo IPv6 peers found. 查看calico的ip池[root@izuf61ygoynhba5hbyzuv0z ~]# calicoctl get ippool -o wideCIDR NAT IPIP192.168.0.0/16 true false10.9.10.0/24 true true fd80:24e2:f998:72d6::/64 false false 删除calico默认的网络(非必要步骤)calicoctl delete ippool 192.168.0.0/16 注意: 这里不是必须要执行的，因为192.168.0.0是我们常用的内网IP, calico默认的网络直接掩掉了我们16位，会造成这个网段网络访问失败，所以删掉这个 创建calico自定义网络不使用IPIP的： cat &lt;&lt; EOF | calicoctl create -f -apiVersion: v1kind: ipPoolmetadata: cidr: 10.8.10.0/24spec: ipip: enabled: false nat-outgoing: true disabled: falseEOF 使用IPIP的: cat &lt;&lt; EOF | calicoctl create -f -apiVersion: v1kind: ipPoolmetadata: cidr: 10.9.10.0/24spec: ipip: enabled: true nat-outgoing: true disabled: falseEOF 注意: 因为我的网络环境问题 下面的例子我都是以使用IPIP的配置。添加之后可以用上面的查看命令检查一下。 docker创建calico网络docker network create --driver=calico --ipam-driver=calico-ipam --subnet=10.9.10.0/24 calico 注意: 很多网上的资料ipam的名字都是calico，那是老版本的，新版本的是calico-ipam。操作之后可以在不同的机器上检查一下，应该都有calico这个网络才对，而且network id应该一致。 [root@izuf61ygoynhba5hbyzuv0z ~]# docker network ls NETWORK ID NAME DRIVER SCOPEc854966102fa bridge bridge local0cd8c3dc5cd1 calico calico globala3cf99e3d5fc host host localee3030094cd6 none null local 配置calico的网络权限cat &lt;&lt; EOF | calicoctl apply -f -- apiVersion: v1 kind: profile metadata: name: calico tags: - calico spec: egress: - action: allow destination: &#123;&#125; source: &#123;&#125; ingress: - action: allow destination: &#123;&#125; source: &#123;&#125;EOF Try Itdocker run -d --net=calico --name=nginx nginx 现在可以用 docker inspect命令看下容器的网络 也可以ping一下~ 相关命令# 1. 查看Calico的Profilecalicoctl get profile -o yaml# 2. 查看Calico当前网络情况calicoctl get workloadEndpoint -o wide# 3. 删除IP池calicoctl delete ippool CIDR# 4. 删除策略配置 calicoctl delete profile profile_name 参考资料 calico的官方API: http://docs.projectcalico.org/v2.0/reference/ calico v1.0的文章 《Ucloud云上环境使用calico+libnetwork连通容器网络实践》https://zhuanlan.zhihu.com/p/24094454","categories":[{"name":"Docker","slug":"Docker","permalink":"http://www.antergone.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.antergone.com/tags/Docker/"}]},{"title":"修改Docker0的IP地址","slug":"修改Docker0的IP地址","date":"2017-03-02T14:40:36.000Z","updated":"2017-08-23T17:16:30.000Z","comments":true,"path":"20170302/修改Docker0的IP地址.html","link":"","permalink":"http://www.antergone.com/20170302/修改Docker0的IP地址.html","excerpt":"","text":"Docker默认使用的网桥 docker0 的网段是 172.17.0.1，正好和公司网络冲突。 网上的资料也一如既往的狗血，在这里记录下自己的配置。 以下步骤自测过得版本: 1.12.x, 1.13.1其他版本，如果有需要可以自行测试，应该差不太多。 第一步 删除原有配置sudo service docker stopsudo ip link set dev docker0 downsudo brctl delbr docker0sudo iptables -t nat -F POSTROUTING 第二步 创建新的网桥sudo brctl addbr docker0sudo ip addr add 192.168.200.1/24 dev docker0sudo ip link set dev docker0 up 第三步 配置Docker的文件注意： 这里是 增加下面的配置 vi /etc/docker/daemon.json&#123; ... \"bip\": \"192.168.200.1/24\", ...&#125; docker的配置现在都是用daemon.json了，不需要去设置DOCKER_OPTS, 所以网络上很多的资料早已过时。 关于daemon.json可以看我之前的文章 https://www.antergone.com/42.html 第四步 重启主机sudo reboot","categories":[{"name":"Docker","slug":"Docker","permalink":"http://www.antergone.com/categories/Docker/"},{"name":"技术杂记","slug":"Docker/技术杂记","permalink":"http://www.antergone.com/categories/Docker/技术杂记/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.antergone.com/tags/Docker/"}]},{"title":"Docker如何安装镜像仓库","slug":"Docker如何安装镜像仓库","date":"2017-02-26T14:46:55.000Z","updated":"2017-08-23T17:12:21.000Z","comments":true,"path":"20170226/Docker如何安装镜像仓库.html","link":"","permalink":"http://www.antergone.com/20170226/Docker如何安装镜像仓库.html","excerpt":"","text":"Docker仓库搭建，其实不是个很难的事情，其实就是跑一个官方的镜像。 docker pull registrymkdir -p /data/docker-registrydocker run -d -p 5000:5000 --restart=always --name registry -v /data/docker-registry/:/var/lib/registry/ registry 但是这里有一个小坑，网上的文档也都是老的。仓库搭建完，docker打好image往仓库推的时候会报下面的错误： Get https://10.46.18.93:5000/v1/_ping: http: server gave HTTP response to HTTPS client 这个错是什么意思呢？ 说白了就是没有证书。所以这个时候如果我们手头没有证书，或者做验证性试验的时候，就需要信任这个地址。 新版本的Docker配置方式是： # 添加仓库地址到 insecure-registriesvim /etc/docker/daemon.json&#123; \"live-restore\": true, \"insecure-registries\":[\"10.46.18.93:5000\"]&#125; 附：daemon.json的全配置项","categories":[{"name":"Docker","slug":"Docker","permalink":"http://www.antergone.com/categories/Docker/"},{"name":"技术杂记","slug":"Docker/技术杂记","permalink":"http://www.antergone.com/categories/Docker/技术杂记/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.antergone.com/tags/Docker/"}]},{"title":"Docker配置文件 daemon.json全配置项","slug":"Docker配置文件 daemon.json全配置项","date":"2017-02-19T14:26:04.000Z","updated":"2017-08-23T17:17:16.000Z","comments":true,"path":"20170219/Docker配置文件 daemon.json全配置项.html","link":"","permalink":"http://www.antergone.com/20170219/Docker配置文件 daemon.json全配置项.html","excerpt":"","text":"截至当前(2017年2月19日)，Docker daemon.json的全配置如下： Linux版本: &#123; \"api-cors-header\": \"\", \"authorization-plugins\": [], \"bip\": \"\", \"bridge\": \"\", \"cgroup-parent\": \"\", \"cluster-store\": \"\", \"cluster-store-opts\": &#123;&#125;, \"cluster-advertise\": \"\", \"debug\": true, \"default-gateway\": \"\", \"default-gateway-v6\": \"\", \"default-runtime\": \"runc\", \"default-ulimits\": &#123;&#125;, \"disable-legacy-registry\": false, \"dns\": [], \"dns-opts\": [], \"dns-search\": [], \"exec-opts\": [], \"exec-root\": \"\", \"fixed-cidr\": \"\", \"fixed-cidr-v6\": \"\", \"graph\": \"\", \"group\": \"\", \"hosts\": [], \"icc\": false, \"insecure-registries\": [], \"ip\": \"0.0.0.0\", \"iptables\": false, \"ipv6\": false, \"ip-forward\": false, \"ip-masq\": false, \"labels\": [], \"live-restore\": true, \"log-driver\": \"\", \"log-level\": \"\", \"log-opts\": &#123;&#125;, \"max-concurrent-downloads\": 3, \"max-concurrent-uploads\": 5, \"mtu\": 0, \"oom-score-adjust\": -500, \"pidfile\": \"\", \"raw-logs\": false, \"registry-mirrors\": [], \"runtimes\": &#123; \"runc\": &#123; \"path\": \"runc\" &#125;, \"custom\": &#123; \"path\": \"/usr/local/bin/my-runc-replacement\", \"runtimeArgs\": [ \"--debug\" ] &#125; &#125;, \"selinux-enabled\": false, \"storage-driver\": \"\", \"storage-opts\": [], \"swarm-default-advertise-addr\": \"\", \"tls\": true, \"tlscacert\": \"\", \"tlscert\": \"\", \"tlskey\": \"\", \"tlsverify\": true, \"userland-proxy\": false, \"userns-remap\": \"\"&#125; Windows版本&#123; \"authorization-plugins\": [], \"bridge\": \"\", \"cluster-advertise\": \"\", \"cluster-store\": \"\", \"debug\": true, \"default-ulimits\": &#123;&#125;, \"disable-legacy-registry\": false, \"dns\": [], \"dns-opts\": [], \"dns-search\": [], \"exec-opts\": [], \"fixed-cidr\": \"\", \"graph\": \"\", \"group\": \"\", \"hosts\": [], \"insecure-registries\": [], \"labels\": [], \"live-restore\": true, \"log-driver\": \"\", \"log-level\": \"\", \"mtu\": 0, \"pidfile\": \"\", \"raw-logs\": false, \"registry-mirrors\": [], \"storage-driver\": \"\", \"storage-opts\": [], \"swarm-default-advertise-addr\": \"\", \"tlscacert\": \"\", \"tlscert\": \"\", \"tlskey\": \"\", \"tlsverify\": true&#125;","categories":[{"name":"Docker","slug":"Docker","permalink":"http://www.antergone.com/categories/Docker/"},{"name":"技术杂记","slug":"Docker/技术杂记","permalink":"http://www.antergone.com/categories/Docker/技术杂记/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.antergone.com/tags/Docker/"}]},{"title":"[Java] Retrofit2.0 如何进行GBK编码","slug":"[Java] Retrofit2.0 如何进行GBK编码","date":"2016-12-22T05:21:24.000Z","updated":"2017-08-23T17:16:06.000Z","comments":true,"path":"20161222/[Java] Retrofit2.0 如何进行GBK编码.html","link":"","permalink":"http://www.antergone.com/20161222/[Java] Retrofit2.0 如何进行GBK编码.html","excerpt":"","text":"对Retrofit + OkHttp还不熟悉的人可以点传送门，先看下这两个东西的使用。 Retrofit: https://github.com/square/retrofit OkHttp: https://github.com/square/okhttp 分析接口文档要求Post请求，字段使用GBK编码我们先按照Retrofit的规范和接口文档来写接口： @POST(SEND_API_URL)@FormUrlEncodedpublic Call&lt;SendResponse&gt; send(@FieldMap Map&lt;String, Object&gt; bodyMap); Retrofit 默认情况下是使用UTF-8编码，其实就是调用的OkHttp3的编码规则。我们来看下OkHttp是怎么做的 //以下代码摘自okhttp3.HttpUrlstatic String canonicalize(String input, int pos, int limit, String encodeSet, boolean alreadyEncoded, boolean strict, boolean plusIsSpace, boolean asciiOnly) &#123; int codePoint; for(int i = pos; i &lt; limit; i += Character.charCount(codePoint)) &#123; codePoint = input.codePointAt(i); if(codePoint &lt; 32 || codePoint == 127 || codePoint &gt;= 128 &amp;&amp; asciiOnly || encodeSet.indexOf(codePoint) != -1 || codePoint == 37 &amp;&amp; (!alreadyEncoded || strict &amp;&amp; !percentEncoded(input, i, limit)) || codePoint == 43 &amp;&amp; plusIsSpace) &#123; Buffer out = new Buffer(); out.writeUtf8(input, pos, i); canonicalize(out, input, i, limit, encodeSet, alreadyEncoded, strict, plusIsSpace, asciiOnly); return out.readUtf8(); &#125; &#125; return input.substring(pos, limit);&#125; 很多文章说，给他设置一个Content-Type，也就是带上Header，在做请求的时候，确实会去解析，但是对于编码来说，其实并没有什么用。 @Headers(\"Content-Type: application/x-www-form-urlencoded;charset=GBK\") 解决方案@POST(SEND_API_URL)@FormUrlEncoded@Headers(\"Content-Type: application/x-www-form-urlencoded;charset=GBK\")public Call&lt;SendResponse&gt; send(@FieldMap(encoded = true) Map&lt;String, Object&gt; bodyMap); 在 @FieldMap 注解里面设置 encoded = true这样做的意思就是，这个Map里面的东西我已经做过编码转换，不需要框架去重新转了。 也就是上面的代码中alreadyEncoded = true但是这里也要注意，在调用这个方法的时候，传入的Map要自己转码。 总结最近在对接第三方接口，真的是千奇百怪，吐槽一天都吐槽不完。作为开发人员，我觉得写对外的接口一定要注意，这个接口是给对方开发人员用的，至少编码、格式、参数加密、鉴权等等都要统一。本是同根生相煎何太急 : )","categories":[{"name":"Java","slug":"Java","permalink":"http://www.antergone.com/categories/Java/"},{"name":"技术杂记","slug":"Java/技术杂记","permalink":"http://www.antergone.com/categories/Java/技术杂记/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.antergone.com/tags/Java/"},{"name":"OkHttp","slug":"OkHttp","permalink":"http://www.antergone.com/tags/OkHttp/"},{"name":"Retrofit","slug":"Retrofit","permalink":"http://www.antergone.com/tags/Retrofit/"}]},{"title":"[Cracked] Burpsuite V1.7.11 破解版","slug":"[Cracked] Burpsuite V1.7.11","date":"2016-12-16T17:23:26.000Z","updated":"2017-08-23T18:15:41.000Z","comments":true,"path":"20161217/[Cracked] Burpsuite V1.7.11.html","link":"","permalink":"http://www.antergone.com/20161217/[Cracked] Burpsuite V1.7.11.html","excerpt":"","text":"之前的版本貌似都集体失效了，这个版本是目前可以用的，刚测试了下。 下载地址中国红客联盟 =&gt; https://ssl.ihonker.org/tools/burpsuite1.7.11.zip","categories":[{"name":"实用工具","slug":"实用工具","permalink":"http://www.antergone.com/categories/实用工具/"}],"tags":[{"name":"Tools","slug":"Tools","permalink":"http://www.antergone.com/tags/Tools/"},{"name":"Hacker","slug":"Hacker","permalink":"http://www.antergone.com/tags/Hacker/"}]},{"title":"[Cracked]Navicat Premium for Mac 11.2.13 破解版","slug":"[Cracked]Navicat Premium for Mac 11.2.13","date":"2016-12-16T16:29:26.000Z","updated":"2017-08-23T16:59:12.000Z","comments":true,"path":"20161217/[Cracked]Navicat Premium for Mac 11.2.13.html","link":"","permalink":"http://www.antergone.com/20161217/[Cracked]Navicat Premium for Mac 11.2.13.html","excerpt":"","text":"Navicat的简介就不多说了，直接上地址下载可用，亲测！ 下载地址:链接: https://pan.baidu.com/s/1qY0aYwG密码: 1m6a","categories":[{"name":"实用工具","slug":"实用工具","permalink":"http://www.antergone.com/categories/实用工具/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://www.antergone.com/tags/Mac/"},{"name":"SQL","slug":"SQL","permalink":"http://www.antergone.com/tags/SQL/"},{"name":"Tools","slug":"Tools","permalink":"http://www.antergone.com/tags/Tools/"}]}]}